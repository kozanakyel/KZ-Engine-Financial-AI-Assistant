{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:16:28.059518800Z",
     "start_time": "2023-06-02T12:16:28.037249600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('T_OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "hugging_api_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hugging_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:16:36.482168600Z",
     "start_time": "2023-06-02T12:16:35.304613100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "                                                  ConversationSummaryMemory,\n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:17:04.111502400Z",
     "start_time": "2023-06-02T12:17:04.092503900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model_name='text-ada-001',\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:17:08.944747100Z",
     "start_time": "2023-06-02T12:17:08.934748200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:17:17.509331800Z",
     "start_time": "2023-06-02T12:17:17.498332700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:17:21.881741800Z",
     "start_time": "2023-06-02T12:17:21.876739100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:17:39.635022900Z",
     "start_time": "2023-06-02T12:17:39.604022100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, Any],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        response = self.generate([inputs], run_manager=run_manager)\n",
      "        return self.create_outputs(response)[0]\n",
      "     def apply(\n",
      "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
      "    ) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        callback_manager = CallbackManager.configure(\n",
      "            callbacks, self.callbacks, self.verbose\n",
      "        )\n",
      "        run_manager = callback_manager.on_chain_start(\n",
      "            {\"name\": self.__class__.__name__},\n",
      "            {\"input_list\": input_list},\n",
      "        )\n",
      "        try:\n",
      "            response = self.generate(input_list, run_manager=run_manager)\n",
      "        except (KeyboardInterrupt, Exception) as e:\n",
      "            run_manager.on_chain_error(e)\n",
      "            raise e\n",
      "        outputs = self.create_outputs(response)\n",
      "        run_manager.on_chain_end({\"outputs\": outputs})\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:18:18.678659300Z",
     "start_time": "2023-06-02T12:18:18.674658300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add buffer memory for conversation chain\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:18:25.025290800Z",
     "start_time": "2023-06-02T12:18:24.551301200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!', 'history': '', 'response': '\\n\\nGood morning!'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:18:39.267412200Z",
     "start_time": "2023-06-02T12:18:38.842007100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 116 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThat's an interesting idea! I'm not sure how it would work, but it's an interesting idea.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Memory de ne varsa her seferinde token olarak openai dan cevap almaya kullaniyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:19:12.206710Z",
     "start_time": "2023-06-02T12:19:11.243618700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 248 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nI can think of a few possibilities where large language models could be integrated with external knowledge. One possibility would be to create a system that could understand complex texts and understand them in a more detail-able way. This would be great for those who need to communicate complex topics. Another possibility is to use the data collected from understanding texts to create new technologies that can help with the field of security. It could be that after understanding the text, the system could produce differently shaped objects or symbols that could be used in security videos or applications.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:19:31.185635700Z",
     "start_time": "2023-06-02T12:19:31.174634700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI: \n",
      "\n",
      "Good morning!\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI: \n",
      "\n",
      "That's an interesting idea! I'm not sure how it would work, but it's an interesting idea.\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI:  \n",
      "\n",
      "I can think of a few possibilities where large language models could be integrated with external knowledge. One possibility would be to create a system that could understand complex texts and understand them in a more detail-able way. This would be great for those who need to communicate complex topics. Another possibility is to use the data collected from understanding texts to create new technologies that can help with the field of security. It could be that after understanding the text, the system could produce differently shaped objects or symbols that could be used in security videos or applications.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:20:30.967144400Z",
     "start_time": "2023-06-02T12:20:30.963143300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:20:40.354142Z",
     "start_time": "2023-06-02T12:20:40.350144500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:21:09.399559100Z",
     "start_time": "2023-06-02T12:21:08.449148400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 255 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nGood morning!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(\n",
    "    conversation_sum,\n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:21:20.934486100Z",
     "start_time": "2023-06-02T12:21:19.977505200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 396 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThat's an interesting idea! I'm not sure if this is the right time or place, but I can give you some ideas on how we could go about this.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum,\n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T12:22:54.337843700Z",
     "start_time": "2023-06-02T12:22:51.932227Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 194\n",
      "Summary memory conversation length: 32\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('text-ada-001')\n",
    "\n",
    "# show number of tokens for the memory used by each memory type\n",
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
